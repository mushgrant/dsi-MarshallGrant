{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "Data pipelines are a series of automated data transformations that ensure the validity of your work for routine data maintenance tasks. Each stage of a pipeline feeds from the previous stage, i.e. the output of a stage is plugged into the input of the next stage and data flows through the pipeline from beginning to end just as water flows through a pipeline. Many organizations rely on data engineering teams to encode common tasks into pipelines.\n",
    "\n",
    "Examples of data transformations:\n",
    "- change in scale, units, or base\n",
    "- text vectorization\n",
    "- image vectorization\n",
    "- sound file vectorization\n",
    "- missing data imputation\n",
    "- clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"stumbleupon.tsv\", sep='\\t')\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "\n",
    "# fill NA with empty cells and check data\n",
    "data\n",
    "titles = data['title'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7395 entries, 0 to 7394\n",
      "Data columns (total 29 columns):\n",
      "url                               7395 non-null object\n",
      "urlid                             7395 non-null int64\n",
      "boilerplate                       7395 non-null object\n",
      "alchemy_category                  7395 non-null object\n",
      "alchemy_category_score            7395 non-null object\n",
      "avglinksize                       7395 non-null float64\n",
      "commonlinkratio_1                 7395 non-null float64\n",
      "commonlinkratio_2                 7395 non-null float64\n",
      "commonlinkratio_3                 7395 non-null float64\n",
      "commonlinkratio_4                 7395 non-null float64\n",
      "compression_ratio                 7395 non-null float64\n",
      "embed_ratio                       7395 non-null float64\n",
      "framebased                        7395 non-null int64\n",
      "frameTagRatio                     7395 non-null float64\n",
      "hasDomainLink                     7395 non-null int64\n",
      "html_ratio                        7395 non-null float64\n",
      "image_ratio                       7395 non-null float64\n",
      "is_news                           7395 non-null object\n",
      "lengthyLinkDomain                 7395 non-null int64\n",
      "linkwordscore                     7395 non-null int64\n",
      "news_front_page                   7395 non-null object\n",
      "non_markup_alphanum_characters    7395 non-null int64\n",
      "numberOfLinks                     7395 non-null int64\n",
      "numwords_in_url                   7395 non-null int64\n",
      "parametrizedLinkRatio             7395 non-null float64\n",
      "spelling_errors_ratio             7395 non-null float64\n",
      "label                             7395 non-null int64\n",
      "title                             7383 non-null object\n",
      "body                              7338 non-null object\n",
      "dtypes: float64(12), int64(9), object(8)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       0\n",
       "5       0\n",
       "6       1\n",
       "7       0\n",
       "8       1\n",
       "9       1\n",
       "10      0\n",
       "11      0\n",
       "12      1\n",
       "13      1\n",
       "14      0\n",
       "15      0\n",
       "16      0\n",
       "17      0\n",
       "18      1\n",
       "19      1\n",
       "20      0\n",
       "21      1\n",
       "22      0\n",
       "23      1\n",
       "24      1\n",
       "25      0\n",
       "26      0\n",
       "27      0\n",
       "28      1\n",
       "29      1\n",
       "       ..\n",
       "7365    1\n",
       "7366    0\n",
       "7367    1\n",
       "7368    1\n",
       "7369    1\n",
       "7370    0\n",
       "7371    1\n",
       "7372    0\n",
       "7373    0\n",
       "7374    0\n",
       "7375    0\n",
       "7376    1\n",
       "7377    0\n",
       "7378    1\n",
       "7379    1\n",
       "7380    0\n",
       "7381    0\n",
       "7382    1\n",
       "7383    1\n",
       "7384    1\n",
       "7385    0\n",
       "7386    1\n",
       "7387    1\n",
       "7388    0\n",
       "7389    0\n",
       "7390    0\n",
       "7391    0\n",
       "7392    1\n",
       "7393    1\n",
       "7394    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set label as target\n",
    "y = data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.51332\n",
       "0    0.48668\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check target proportion\n",
    "y.value_counts()/ len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8',\n",
       "        input=u'IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countvectorize our first title\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000, ngram_range=(1,2), stop_words='english', binary=True) # we link this later to the pipeline\n",
    "vectorizer.fit(['IBM Sees Holographic Calls Air Breathing'])\n",
    "\n",
    "#CountVectorizer(data['title'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'air': 0,\n",
       " u'air breathing': 1,\n",
       " u'breathing': 2,\n",
       " u'calls': 3,\n",
       " u'calls air': 4,\n",
       " u'holographic': 5,\n",
       " u'holographic calls': 6,\n",
       " u'ibm': 7,\n",
       " u'ibm sees': 8,\n",
       " u'sees': 9,\n",
       " u'sees holographic': 10}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how Count Vectorizer works:\n",
    "![Example](assets/CountVectorizer.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'air',\n",
       " u'air breathing',\n",
       " u'breathing',\n",
       " u'calls',\n",
       " u'calls air',\n",
       " u'holographic',\n",
       " u'holographic calls',\n",
       " u'ibm',\n",
       " u'ibm sees',\n",
       " u'sees',\n",
       " u'sees holographic']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get n-grams\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize our original training title\n",
    "vectorizer.transform(['IBM Sees Holographic Air']).todense()  #here were comparing a new title with our original parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `transform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles)\n",
    "#print X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores: [ 0.74574209  0.75659229  0.75487013]\n"
     ]
    }
   ],
   "source": [
    "# build Logit and CV score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "scores = cross_val_score(model, X,y)\n",
    "print ('CV scores: {}'.format(scores)) # accuracy of our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46801082,  0.53198918],\n",
       "       [ 0.28316375,  0.71683625],\n",
       "       [ 0.00513772,  0.99486228],\n",
       "       ..., \n",
       "       [ 0.2906286 ,  0.7093714 ],\n",
       "       [ 0.60684225,  0.39315775],\n",
       "       [ 0.6632032 ,  0.3367968 ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into a training set\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "training_data = data[:6000]\n",
    "X_train = training_data['title'].fillna('')\n",
    "y_train = training_data['label']\n",
    "\n",
    "\n",
    "# reserve future data, unavailable at training time\n",
    "X_new = data[6000:]['title'].fillna('')\n",
    "\n",
    "# Fit the full pipeline\n",
    "pipeline = Pipeline([('vec', vectorizer),                 #linked from vectorizer earlier\n",
    "                    ('model', model)])\n",
    "\n",
    "# This means we perform the steps laid out above\n",
    "# First we fit the vectorizer,\n",
    "# And then feed the output of that into the fit function of the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Here again we apply the full pipeline for predictions\n",
    "# The text is transformed automatically to match the features from the pipeline\n",
    "pipeline.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Feature Sets in Pipelines\n",
    "\n",
    "We may want to merge many different feature sets automatically. Here we can use scikit-learn's `FeatureUnion`.\n",
    "\n",
    "While scikit-learn pipelines help with managing the transformation from raw data, there may be many steps before this takes place in your pipeline. These pipelines are often referred to as ETL pipelines for (Extract, Transform, Load). In an ETL pipeline, the data is pulled or extracted from some source (like a database), transformed or manipulated, and then loaded into whatever system will analyze the data.\n",
    "\n",
    "Many data science teams rely on software tools to manage these ETL pipelines. If a transformation step fails, these tools alert you, or ensure that step can be re-run. If these transformation steps need to happen daily or weekly, these tools can manage that timeline.\n",
    "\n",
    "One of the most popular Python tools for this is [Luigi](https://github.com/spotify/luigi) developed by Spotify.\n",
    "An alternative is [Airflow](https://github.com/airbnb/airflow) by AirBnB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# test `make_pipeline` vs `Pipeline`; are they different?\n",
    "pipe1 = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "pipe2 = Pipeline(steps = [('stansdardscaler', StandardScaler()),\n",
    "                         ('logistic_regr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('stansdardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logistic_regr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2 (no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "In pairs, assign one function to each pair, they have to read about it in the doc and then explain it to the class.\n",
    "\n",
    "1. Binarizer\n",
    "1. KernelCenterer\n",
    "1. MaxAbsScaler\n",
    "1. MinMaxScaler\n",
    "1. Normalizer\n",
    "1. OneHotEncoder\n",
    "1. PolynomialFeatures\n",
    "1. RobustScaler\n",
    "1. StandardScaler\n",
    "1. Data Imputation\n",
    "\n",
    "1. Imputer\n",
    "1. Function Transformer\n",
    "\n",
    "1. FunctionTransformer\n",
    "1. Label Manipulators\n",
    "\n",
    "1. LabelBinarizer\n",
    "1. LabelEncoder\n",
    "1. MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# implement custom transformers by extending the BaseClass in sklearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FeatureMultiplier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        return X * self.factor\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "fm = FeatureMultiplier(2)\n",
    "\n",
    "test = np.diag((1,2,3,4))\n",
    "print test\n",
    "\n",
    "fm.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with `FunctionTransformer` from the preprocessing module?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Implement a custom transformer that selects a specific feature from a Pandas dataframe. It should be initialized with the column name or the column index and it should return the selected column when transforming a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisit the salary prediction lab. How could you use `make_pipeline` and `make_union` to build a pipeline that performs the same steps all in one pass?\n",
    "\n",
    "You will have to build something like this:\n",
    "\n",
    ">Data: SelectCategoricalFeaturesTransformer: OneHotEncoder: FeatureUnion: Model: SelectNumericalFeaturesTransformer: Scaler\n",
    "\n",
    "Students:\n",
    "- Review lab and identify the steps that were performed\n",
    "- For each step, determine input and output\n",
    "- Is the input the whole dataframe or only a subset of the features?\n",
    "- Is the output new features or a prediction?\n",
    "- Identify what kind of transformer is needed:\n",
    "    - Is it a custom transformer?\n",
    "    - Does scikit-learn provide a transformer like this out of the box?\n",
    "- If features are treated differently, how do we recombine ([Feature Union](http://scikit-learn.org/stable/modules/pipeline.html)) them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
